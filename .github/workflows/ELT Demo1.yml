name: ELT Pipeline Check

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  check-elt-pipeline:
    name: Check ELT Pipeline
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
    
      - name: Install dependencies
        run: | 
         python -m pip install --upgrade pip
         pip install -r requirements.txt
          
      #- name: Install Dependencies
       # run: |
          #python -m pip install --upgrade pip
          #if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          #pip install flake8 black pytest sqlfluff apache-airflow
          #pip install dbt-core dbt-postgres  # change to dbt-bigquery/dbt-snowflake if needed

          #python -m pip install --upgrade pip
          #if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          #if [ -f pyproject.toml ]; then pip install .; fi

      - name: Lint Python
        run: flake8 .

      #- name: Lint with flake8
        #run: |
          #pip install flake8
          #flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics

     # - name: Run Unit Tests with Pytest
        #run: |
         # pip install pytest
         # pytest tests/

      - name: Run tests with pytest
        run: |
          pytest --maxfail=5 --disable-warnings -q

      # Optional: dbt checks
      #- name: Run dbt debug (if using dbt)
       # run: |
         # if command -v dbt >/dev/null 2>&1; then dbt debug; fi
      
      # Optional: dbt checks for MS SQL
      - name: Run dbt debug (with MS SQL)
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml <<EOL
          my_project:
            target: dev
            outputs:
              dev:
                type: sqlserver
                driver: "ODBC Driver 18 for SQL Server"
                server: ${{ secrets.DB_SERVER_SECRET }}
                port: 1433
                user: ${{ secrets.DB_USER_SECRET }}
                password: ${{ secrets.DB_PASSWORD_SECRET }}
                database: ${{ secrets.DB_NAME_SECRET }}
                schema: dbo
                trust_cert: true
            EOL
            dbt debug


      # Optional: Airflow DAG validation
      #- name: Validate Airflow DAGs (if using Airflow)
      #  run: |
       #   if [ -d dags ]; then
        #    python -m compileall dags/
        #  fi

      - name: Deep Validate Airflow DAGs (Full Airflow Check)
        run: |
          if [ -d dags ]; then
          echo "🟡 DAGs folder found → Running Airflow DAG validation..."
          pip install apache-airflow
          airflow db init

          for dag in $(ls dags/*.py); do
          echo "🔍 Validating $dag ..."
          airflow dags validate $(basename $dag .py) || exit 1
          done

          echo "✅ All DAGs valid and loadable by Airflow."
          else
          echo "ℹ️ No dags/ folder found. Skipping Airflow validation."
          fi


      # Optional: Custom data validation script
      - name: Run Data Validation Script
        run: |
          if [ -f scripts/validate_data.py ]; then
            python scripts/validate_data.py
          fi
