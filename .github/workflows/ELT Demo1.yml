name: ELT Pipeline Check

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  check-elt-pipeline:
    name: Check ELT Pipeline
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
    
      - name: Install dependencies
        run: | 
         python -m pip install --upgrade pip
         pip install -r requirements.txt
          
      #- name: Install Dependencies
       # run: |
          #python -m pip install --upgrade pip
          #if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          #pip install flake8 black pytest sqlfluff apache-airflow
          #pip install dbt-core dbt-postgres  # change to dbt-bigquery/dbt-snowflake if needed

          #python -m pip install --upgrade pip
          #if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          #if [ -f pyproject.toml ]; then pip install .; fi

      - name: Lint Python
        run: flake8 .

      #- name: Lint with flake8
        #run: |
          #pip install flake8
          #flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics

     # - name: Run Unit Tests with Pytest
        #run: |
         # pip install pytest
         # pytest tests/

      - name: Run tests with pytest
        run: |
          pytest --maxfail=5 --disable-warnings -q

      # Optional: dbt checks
      #- name: Run dbt debug (if using dbt)
       # run: |
         # if command -v dbt >/dev/null 2>&1; then dbt debug; fi
      
      # Optional: dbt checks for MS SQL
      - name: Run dbt debug (with MS SQL)
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml <<EOL
          my_project:
            target: dev
            outputs:
              dev:
                type: sqlserver
                driver: "ODBC Driver 18 for SQL Server"
                server: ${{ secrets.DB_SERVER_SECRET }}
                port: 1433
                user: ${{ secrets.DB_USER_SECRET }}
                password: ${{ secrets.DB_PASSWORD_SECRET }}
                database: ${{ secrets.DB_NAME_SECRET }}
                schema: dbo
                trust_cert: true
            EOL
            dbt debug


      # Optional: Airflow DAG validation
      #- name: Validate Airflow DAGs (if using Airflow)
      #  run: |
       #   if [ -d dags ]; then
        #    python -m compileall dags/
        #  fi

      #- name: Deep Validate Airflow DAGs (Full Airflow Check)
        #run: |
          #if [ -d dags ]; then
          #echo "ðŸŸ¡ DAGs folder found â†’ Running Airflow DAG validation..."
          #pip install apache-airflow
         #airflow db init

          #for dag in $(ls dags/*.py); do
         # echo "ðŸ” Validating $dag ..."
         # airflow dags validate $(basename $dag .py) || exit 1
         # done

          #echo "âœ… All DAGs valid and loadable by Airflow."
          #else
          #echo "â„¹ï¸ No dags/ folder found. Skipping Airflow validation."
          #fi
            
      - name: Validate Airflow DAGs
        run: |
         echo "ðŸ” Validating Airflow DAG imports..."
         python - << 'EOF'
         from airflow.models import DagBag

         dag_bag = DagBag(dag_folder="dags/", include_examples=False)

         if dag_bag.import_errors:
          print("âŒ DAG import errors found:")
         for dag_id, error in dag_bag.import_errors.items():
          print(f"{dag_id} -> {error}")
         exit(1)
         else:
         print("âœ… DAGs loaded successfully â€” no syntax/import errors.")
         exit(0)
         EOF


      # Optional: Custom data validation script
      - name: Run Data Validation Script
        run: |
          if [ -f scripts/validate_data.py ]; then
            python scripts/validate_data.py
          fi
